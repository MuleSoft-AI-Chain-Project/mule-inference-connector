<?xml version="1.0" encoding="UTF-8"?>

<mule xmlns:ee="http://www.mulesoft.org/schema/mule/ee/core"
	xmlns:mac-inference="http://www.mulesoft.org/schema/mule/mac-inference" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:munit="http://www.mulesoft.org/schema/mule/munit" xmlns:munit-tools="http://www.mulesoft.org/schema/mule/munit-tools"
	xmlns="http://www.mulesoft.org/schema/mule/core" xmlns:doc="http://www.mulesoft.org/schema/mule/documentation" xsi:schemaLocation="
		http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
		http://www.mulesoft.org/schema/mule/munit http://www.mulesoft.org/schema/mule/munit/current/mule-munit.xsd
		http://www.mulesoft.org/schema/mule/munit-tools  http://www.mulesoft.org/schema/mule/munit-tools/current/mule-munit-tools.xsd
http://www.mulesoft.org/schema/mule/mac-inference http://www.mulesoft.org/schema/mule/mac-inference/current/mule-mac-inference.xsd
http://www.mulesoft.org/schema/mule/ee/core http://www.mulesoft.org/schema/mule/ee/core/current/mule-ee.xsd">	
		<munit:config name="agent-define-prompt-template.xml">
		<munit:parameterizations >
			<munit:parameterization name="config-openai" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="OpenAIConfig" />
					<munit:parameter propertyName="llmModel" value="${openai.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-mistralai" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="MistralAIConfig" />
					<munit:parameter propertyName="llmModel" value="${mistral.llmModel}" />
					<munit:parameter propertyName="inputCount" value="119" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-openrouter" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="OpenrouterConfig" />
					<munit:parameter propertyName="llmModel" value="${openrouter.llmModel}" />
					<munit:parameter propertyName="inputCount" value="19" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-openai-compatible" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="OpenAICompatibleConfig" />
					<munit:parameter propertyName="llmModel" value="${openai-compatible.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-huggingface" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="HuggingFaceConfig" />
					<munit:parameter propertyName="llmModel" value="${huggingface.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-groq" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="GroqConfig" />
					<munit:parameter propertyName="llmModel" value="${groq.llmModel}" />
					<munit:parameter propertyName="inputCount" value="128" />
				</munit:parameters>
			</munit:parameterization>
			<!--<munit:parameterization name="config-ibm-watson" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="IBMWatsonConfig" />
					<munit:parameter propertyName="llmModel" value="${ibm-watson.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
				</munit:parameters>
			</munit:parameterization> -->
			<munit:parameterization name="config-ollama" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="OllamaConfig" />
					<munit:parameter propertyName="llmModel" value="${ollama.llmModel}" />
					<munit:parameter propertyName="inputCount" value="136" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-github" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="GithubConfig" />
					<munit:parameter propertyName="llmModel" value="${github.llmModel}" />
					<munit:parameter propertyName="model" value="${github.model}" />
					<munit:parameter propertyName="inputCount" value="123" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-cerebras" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="CerebrasConfig" />
					<munit:parameter propertyName="llmModel" value="${cerebras.llmModel}" />
					<munit:parameter propertyName="inputCount" value="148" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-nvidia" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="NvidiaConfig" />
					<munit:parameter propertyName="llmModel" value="${nvidia.llmModel}" />
					<munit:parameter propertyName="inputCount" value="128" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-xai" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="XaiConfig" />
					<munit:parameter propertyName="llmModel" value="${xai.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-anthropic" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="AnthropicConfig" />
					<munit:parameter propertyName="llmModel" value="${anthropic.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-ai21labs" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="Ai21labsConfig" />
					<munit:parameter propertyName="llmModel" value="${ai21labs.llmModel}" />
					<munit:parameter propertyName="inputCount" value="128" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-cohere" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="CohereConfig" />
					<munit:parameter propertyName="llmModel" value="${cohere.llmModel}" />
					<munit:parameter propertyName="inputCount" value="113" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-xinference" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="XinferenceConfig" />
					<munit:parameter propertyName="llmModel" value="${xinference.llmModel}" />
					<munit:parameter propertyName="inputCount" value="120" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-zhipu" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="ZhipuConfig" />
					<munit:parameter propertyName="llmModel" value="${zhipu.llmModel}" />
					<munit:parameter propertyName="inputCount" value="120" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-portkey" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="PortkeyConfig" />
					<munit:parameter propertyName="llmModel" value="${portkey.llmModel}" />
					<munit:parameter propertyName="inputCount" value="119" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-llmapi" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="LlmApiConfig" />
					<munit:parameter propertyName="llmModel" value="${llmapi.llmModel}" />
					<munit:parameter propertyName="inputCount" value="128" />
				</munit:parameters>
			</munit:parameterization>
		</munit:parameterizations>
	</munit:config>
	<munit:test name="agent-define-prompt-template-Test" doc:id="604c749a-3b6f-46e5-ae28-d01417c10ba2" description="Test">
		<munit:execution >
			<flow-ref doc:name="Flow-ref to PROMPT_TEMPLATE_OPERATION" doc:id="b309cec5-9bf3-4227-b08f-7a28b194215b" name="agent_define_prompt_template"/>
			<logger level="INFO" doc:name="Logger" doc:id="84b44d0d-5770-400f-9aa2-c1aa78f19543" message="#[payload]" />
		</munit:execution>
		<munit:validation >
			<munit-tools:assert-that doc:name="payload.response" doc:id="ce90e174-c89f-4564-a9ec-dfb99ad244c4" message="Payload response is wrong answer" expression="#[payload.payload.response]" is="#[MunitTools::containsString('positive')]" />
			<munit-tools:assert-equals doc:name="Input Token" doc:id="3321379b-64c8-4e79-8b87-70d1cecd3c07" actual="#[attributes.tokenUsage.inputCount as String]" expected="${inputCount}" message="Incorrect Input Token" />
			<munit-tools:assert-equals doc:name="model Info" doc:id="8d66f734-342b-4913-8960-d1aa81a7620f" actual="#[attributes.additionalAttributes.model]" expected="${llmModel}" message="Incorrect Model Info" />
		</munit:validation>
	</munit:test>
	<sub-flow name="agent_define_prompt_template" doc:id="f3be6666-ba40-43cc-808f-f28145890560" >
		<set-variable value="#[%dw 2.0&#10;   output application/json&#10;   ---&#10;   {&#10;    &quot;template&quot;: &quot;You are a customer satisfaction agent, who analyses the customer feedback in the dataset. Answer via json output and add a type for the result only with positive or negative as well as the complete answer&quot;,&#10;    &quot;instructions&quot;:&quot;If the customer feedback in the dataset is negative, open a service satisfaction case and apologize to the customer. If the customer feedback in the dataset is positive, thank the customer and wish them a nice day. Don't repeat the feedback and be more direct starting the conversation with formal greetings&quot;,&#10;    &quot;dataset&quot;: &quot;The training last week was amazing, we learned so much and the trainer was very friendly.&quot;&#10;	}]" doc:name="Set Variable" doc:id="a6216448-330f-4de3-858e-d6776b89e0a0" variableName="testPayload" />
		<mac-inference:agent-define-prompt-template doc:name="[Agent] Define Prompt Template" doc:id="556dde58-3eaa-4691-b8af-8ae3a51bf994" config-ref="${config}">
			<mac-inference:template ><![CDATA[#[vars.testPayload.template]]]></mac-inference:template>
			<mac-inference:instructions ><![CDATA[#[vars.testPayload.instructions]]]></mac-inference:instructions>
			<mac-inference:data ><![CDATA[#[vars.testPayload.dataset]]]></mac-inference:data>
		</mac-inference:agent-define-prompt-template>
		<ee:transform doc:name="Transform Message" doc:id="59da2965-d4f8-4a69-b64a-65dd1e233d61" >
			<ee:message >
				<ee:set-payload ><![CDATA[%dw 2.0
					output application/json
					---
					{
						payload: payload,
						attributes: attributes
					}]]></ee:set-payload>
			</ee:message>
		</ee:transform>
	</sub-flow>

</mule>
