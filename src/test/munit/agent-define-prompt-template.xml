<?xml version="1.0" encoding="UTF-8"?>

<mule xmlns:ee="http://www.mulesoft.org/schema/mule/ee/core"
	xmlns:mac-inference="http://www.mulesoft.org/schema/mule/mac-inference" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
	xmlns:munit="http://www.mulesoft.org/schema/mule/munit" xmlns:munit-tools="http://www.mulesoft.org/schema/mule/munit-tools"
	xmlns="http://www.mulesoft.org/schema/mule/core" xmlns:doc="http://www.mulesoft.org/schema/mule/documentation" xsi:schemaLocation="
		http://www.mulesoft.org/schema/mule/core http://www.mulesoft.org/schema/mule/core/current/mule.xsd
		http://www.mulesoft.org/schema/mule/munit http://www.mulesoft.org/schema/mule/munit/current/mule-munit.xsd
		http://www.mulesoft.org/schema/mule/munit-tools  http://www.mulesoft.org/schema/mule/munit-tools/current/mule-munit-tools.xsd
http://www.mulesoft.org/schema/mule/mac-inference http://www.mulesoft.org/schema/mule/mac-inference/current/mule-mac-inference.xsd
http://www.mulesoft.org/schema/mule/ee/core http://www.mulesoft.org/schema/mule/ee/core/current/mule-ee.xsd">	
		<munit:config name="agent-define-prompt-template.xml">
		<munit:parameterizations >
			<munit:parameterization name="config-ai21labs" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="Ai21labsConfig" />
					<munit:parameter propertyName="llmModel" value="${ai21labs.llmModel}" />
					<munit:parameter propertyName="inputCount" value="122" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-anthropic" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="AnthropicConfig" />
					<munit:parameter propertyName="llmModel" value="${anthropic.llmModel}" />
					<munit:parameter propertyName="inputCount" value="137" />
					<munit:parameter propertyName="finishReason" value="end_turn" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-azure-ai-foundry" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="azureAiFoundryConfig" />
					<munit:parameter propertyName="llmModel" value="${azure-ai-foundry.llmModel}" />
					<munit:parameter propertyName="inputCount" value="117" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-azure-openai" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="AzureOpenAIConfig" />
					<munit:parameter propertyName="llmModel" value="${azure-openai.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-cerebras" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="CerebrasConfig" />
					<munit:parameter propertyName="llmModel" value="${cerebras.llmModel}" />
					<munit:parameter propertyName="inputCount" value="147" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-cohere" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="CohereConfig" />
					<munit:parameter propertyName="llmModel" value="${cohere.llmModel}" />
					<munit:parameter propertyName="inputCount" value="112" />
					<munit:parameter propertyName="finishReason" value="COMPLETE" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-deepinfra" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="DeepinfraConfig" />
					<munit:parameter propertyName="llmModel" value="${deepinfra.llmModel}" />
					<munit:parameter propertyName="inputCount" value="127" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<!-- Deepseek integrations might not work in local due to sever restrictions, but works with jenkins pipeline-->
			<munit:parameterization name="config-deepseek" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="DeepseekConfig" />
					<munit:parameter propertyName="llmModel" value="${deepseek.llmModel}" />
					<munit:parameter propertyName="inputCount" value="115" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-fireworks" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="FireworksConfig" />
					<munit:parameter propertyName="llmModel" value="${fireworks.llmModel}" />
					<munit:parameter propertyName="inputCount" value="127" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-github" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="GithubConfig" />
					<munit:parameter propertyName="llmModel" value="${github.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-groq" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="GroqConfig" />
					<munit:parameter propertyName="llmModel" value="${groq.llmModel}" />
					<munit:parameter propertyName="inputCount" value="147" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-hugging-face" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="HuggingFaceConfig" />
					<munit:parameter propertyName="llmModel" value="${hugging-face.llmModel}" />
					<munit:parameter propertyName="inputCount" value="121" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-llmapi" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="llmapiConfig" />
					<munit:parameter propertyName="llmModel" value="${llmapi.llmModel}" />
					<munit:parameter propertyName="inputCount" value="147" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-mistralai" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="MistralAIConfig" />
					<munit:parameter propertyName="llmModel" value="${mistral.llmModel}" />
					<munit:parameter propertyName="inputCount" value="118" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-nvidia">
				<munit:parameters>
					<munit:parameter propertyName="config" value="NvidiaConfig"/>
					<munit:parameter propertyName="llmModel" value="${nvidia.llmModel}"/>
					<munit:parameter propertyName="inputCount" value="127"/>
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-openai" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="OpenAIConfig" />
					<munit:parameter propertyName="llmModel" value="${openai.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-openai-compatible">
				<munit:parameters>
					<munit:parameter propertyName="config" value="OpenAICompatibleConfig"/>
					<munit:parameter propertyName="llmModel" value="${openai-compatible.llmModel}"/>
					<munit:parameter propertyName="inputCount" value="123"/>
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-openrouter" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="OpenrouterConfig" />
					<munit:parameter propertyName="llmModel" value="${openrouter.llmModel}" />
					<munit:parameter propertyName="inputCount" value="117" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-perplexity" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="PerplexityConfig" />
					<munit:parameter propertyName="llmModel" value="${perplexity.llmModel}" />
					<munit:parameter propertyName="inputCount" value="112" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-portkey" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="PortkeyConfig" />
					<munit:parameter propertyName="llmModel" value="${portkey.llmModel}" />
					<munit:parameter propertyName="inputCount" value="118" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-together" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="TogetherConfig" />
					<munit:parameter propertyName="llmModel" value="${together.llmModel}" />
					<munit:parameter propertyName="inputCount" value="147" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-vertexai-express" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="VertexAiExpressConfig" />
					<munit:parameter propertyName="llmModel" value="${vertexaiexpress.llmModel}" />
					<munit:parameter propertyName="inputCount" value="112" />
					<munit:parameter propertyName="finishReason" value="STOP" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-xai" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="XAiConfig" />
					<munit:parameter propertyName="llmModel" value="${xai.llmModel}" />
					<munit:parameter propertyName="inputCount" value="123" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-xinference" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="XInferenceConfig" />
					<munit:parameter propertyName="llmModel" value="${xinference.modelResponse}" />
					<munit:parameter propertyName="inputCount" value="125" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
			<munit:parameterization name="config-zhipu" >
				<munit:parameters >
					<munit:parameter propertyName="config" value="ZhipuConfig" />
					<munit:parameter propertyName="llmModel" value="${zhipu.llmModel}" />
					<munit:parameter propertyName="inputCount" value="119" />
					<munit:parameter propertyName="finishReason" value="stop" />
				</munit:parameters>
			</munit:parameterization>
		</munit:parameterizations>
	</munit:config>
	<munit:test name="agent-define-prompt-template-Test" doc:id="aa9159ad-0dca-48bc-a598-dc05a660c05c" description="Test">
		<munit:execution >
			<flow-ref doc:name="Flow-ref to PROMPT_TEMPLATE_OPERATION" doc:id="5b407e10-0484-45f0-8fce-cafe8c6cd720" name="agent_define_prompt_template"/>
			<logger level="INFO" doc:name="Logger" doc:id="ec7f0957-96f3-4331-ad34-6e5fca8c5308" message="#[payload]" />
		</munit:execution>
		<munit:validation >
			<munit-tools:assert-that doc:name="payload.response" doc:id="5fce7ff3-6ed5-4fd9-89f2-7f9f5150eb6e" message="Payload response is wrong answer" expression="#[payload.payload.response]" is="#[MunitTools::containsString('positive')]" />
			<munit-tools:assert-equals doc:name="Input Token" doc:id="a3e4733f-65fe-4d4e-ad9c-7e4d4bf1442d" actual="#[attributes.tokenUsage.inputCount as String]" expected="${inputCount}" message="Incorrect Input Token" />
			<munit-tools:assert-that doc:name="Model Info" doc:id="f688a025-3466-4afe-bacb-6a2275258f96" message="Incorrect Model Info" expression="#[attributes.additionalAttributes.model]" is="#[MunitTools::containsString('${llmModel}')]" />
			<munit-tools:assert-equals doc:name="finish reason" doc:id="3c009ca9-2124-4527-8e0f-7ec87b3c4538" actual="#[attributes.additionalAttributes.finishReason]" expected="${finishReason}" message="Incorrect finish reason" />
		</munit:validation>
	</munit:test>
	<sub-flow name="agent_define_prompt_template" doc:id="a474f4ff-286c-4810-bb5e-8cd5a3f643bf" >
		<set-variable value='#[%dw 2.0
			output application/json
			---
			{
			"template": "You are a customer satisfaction agent, who analyses the customer feedback in the dataset. Answer via json output and add a type for the result only with positive or negative as well as the complete answer",
			"instructions":"If the customer feedback in the dataset is negative, open a service satisfaction case and apologize to the customer. If the customer feedback in the dataset is positive, thank the customer and wish them a nice day. Do not repeat the feedback and be more direct starting the conversation with formal greetings",
			"dataset": "The training last week was amazing, we learned so much and the trainer was very friendly"
		}]' doc:name="Set Variable" doc:id="9c40489f-6ff6-4bbc-90ae-50c8873a124b" variableName="testPayload"/>
		<mac-inference:agent-define-prompt-template doc:name="[Agent] Define Prompt Template" doc:id="d4f74b68-3202-4154-b0ee-1489ef42e870" config-ref="${config}">
			<mac-inference:template ><![CDATA[#[vars.testPayload.template]]]></mac-inference:template>
			<mac-inference:instructions ><![CDATA[#[vars.testPayload.instructions]]]></mac-inference:instructions>
			<mac-inference:data ><![CDATA[#[vars.testPayload.dataset]]]></mac-inference:data>
		</mac-inference:agent-define-prompt-template>
		<ee:transform doc:name="Transform Message" doc:id="bba2fcbe-2617-497f-898f-b4b2e94c8c6f" >
			<ee:message >
				<ee:set-payload ><![CDATA[%dw 2.0
					output application/json
					---
					{
						payload: payload,
						attributes: attributes
					}]]></ee:set-payload>
			</ee:message>
		</ee:transform>
	</sub-flow>

</mule>
